{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "###   RESAMPLING METHODS   ###\n",
    "##############################\n",
    "\n",
    "### TRAIN/TEST SPLIT\n",
    "\n",
    "from random import seed, randrange\n",
    "\n",
    "# Split a dataset into train/test\n",
    "def train_test_split(dataset, split=0.60):\n",
    "    train = list()\n",
    "    train_size = split * len(dataset)\n",
    "    dataset_copy = list(dataset)\n",
    "    while len(train) < train_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        train.append(dataset_copy.pop(index))\n",
    "    return train, dataset_copy\n",
    "\n",
    "#  seed(1)\n",
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13]]\n",
    "\n",
    "train, test = train_test_split(dataset)\n",
    "print(train)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-FOLD CROSS VALIDATION SPLIT\n",
    "\n",
    "# value of K should be divisible by the number of rows in training dataset\n",
    "# to ensure that K-groups has the same number of rows. K should be chosen \n",
    "# so that each group will still be representative of the original dataset.\n",
    "# 3 is a good default for small datasets and 10 is good for large dataset.\n",
    "\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / folds)\n",
    "    for _ in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "folds = cross_validation_split(dataset)\n",
    "print(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.740%\n"
     ]
    }
   ],
   "source": [
    "### Using Train/Test Split for Diabetes Dataset\n",
    "\n",
    "import pandas as p\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = p.read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"Accuracy: %.3f%%\" % (result*100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.216% (4.968%)\n"
     ]
    }
   ],
   "source": [
    "#### Using Cross Validation for Diabetes Dataset\n",
    "## The first 10 lines of code in cell 3 are the same for this cell\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, \n",
    "                              random_state=seed, \n",
    "                              shuffle=True\n",
    "                             )\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, \n",
    "                                     results.std()*100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.604% (41.689%)\n"
     ]
    }
   ],
   "source": [
    "#### Leave Out One Cross Validation\n",
    "## The first 9 lines of code in cell 3 are the same for this cell.\n",
    "\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "loocv = model_selection.LeaveOneOut()\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=loocv)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, \n",
    "                                     results.std()*100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.535% (2.235%)\n"
     ]
    }
   ],
   "source": [
    "#### Repeated Random Test-Train Splits\n",
    "## The first 11 lines of code in cell 3 are the same for this cell\n",
    "\n",
    "num_instances = len(X)\n",
    "kfold = model_selection.ShuffleSplit(n_splits=10, \n",
    "                                     test_size=test_size, \n",
    "                                     random_state=seed\n",
    "                                    )\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, \n",
    "                                     results.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### What Techniques to Use And When\n",
    "\n",
    "## Generally k-fold cross validation is the gold-standard for evaluating \n",
    "#  the performance of a machine learning algorithm on unseen data with \n",
    "#  k set to 3, 5, or 10.\n",
    "\n",
    "## Using a train/test split is good for speed when using a slow algorithm \n",
    "#  and produces performance estimates with lower bias when using large \n",
    "#  datasets.\n",
    "\n",
    "## Techniques like leave-one-out cross validation and repeated random \n",
    "#  splits can be useful intermediates when trying to balance variance \n",
    "#  in the estimated performance, model training speed and dataset size.\n",
    "\n",
    "\n",
    "## The best advice is to experiment and find a technique for your problem \n",
    "#  that is fast and produces reasonable estimates of performance that you \n",
    "#  can use to make decisions. If in doubt, use 10-fold cross validation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
